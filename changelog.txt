1. Gradient Boosted Machine (94.018773076136453 %)
  Number of estimators: 100
  Max depth of estimator: 3
  Learning rate: 1.0

  Feature selection:
    Description:
      Use the 25 categorical features found in projects.csv binarized with LabelEncoder
      
    Reasoning:
      Just to test if things work
      Don't need to use one-hot encoding since we're using decision trees

    Future:
      "Different kinds of models have different advantages. The boosted trees model is very good at handling tabular data with numerical features, or categorical features with fewer than hundreds of categories. One important note is that tree based models are not designed to work with very sparse features. When dealing with sparse input data (e.g. categorical features with large dimension), we can either pre-process the sparse features to generate numerical statistics, or switch to a linear model, which is better suited for such scenarios."
        - Source: http://blog.dato.com/using-gradient-boosted-trees-to-predict-bike-sharing-demand

      Some of our categories are quite sparse, so we should generate statistics (frequency?) for the categorical features
        - Source: https://www.kaggle.com/c/caterpillar-tube-pricing/forums/t/15748/strategies-to-encode-categorical-variables-with-many-categories

      Look up shruken averages (apparently prevents overfitting)
        - Source: https://github.com/rkirana/kdd2014/blob/master/KDDWinningSubmission.pdf

  Training/Validation split:
    Description:
      Training data before 2014
      Did not omit data before April 2010 unlike top submissions
      Select validation set randomly from training data with 70/30 training/validation split

    Reasoning:
      70/30 split seems to be a commonly used ratio
      Random sampling usually the best
     
    Future: 
      Since we have data ordered by time, perhaps we should consider using more recent data for validation?

2. Gradient Boosted Machine (94.086588660803672 %)
  Number of estimators: 100
  Max depth of estimator: 7
  Learning rate: 0.01
  Max number of features to split on: 11

  Feature selection:
    Description:
      Use the 25 categorical features found in projects.csv
      Binarized by replacing the categories with their frequencies

    Reasoning:
      See above future changes

    Future:
      Look up shruken averages (apparently prevents overfitting)
        - Source: https://github.com/rkirana/kdd2014/blob/master/KDDWinningSubmission.pdf

  Training/Validation split:
    Same as above